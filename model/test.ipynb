{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dandreas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from saveAndLoad import *\n",
    "from functools import partial\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_test(Dataset):\n",
    "    def __init__(self):\n",
    "        tlong = lambda x: torch.tensor(x, dtype=torch.long)\n",
    "        tlong2d = lambda x: torch.tensor([x], dtype=torch.long)\n",
    "        tfloat = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "        tfloat2d = lambda x: torch.tensor([x], dtype=torch.float32)\n",
    "        randemb = lambda: torch.rand(640)\n",
    "        \n",
    "        self.cancer = [tlong2d(1), tlong2d(3)]\n",
    "        self.cancer_d = [tlong2d(3), tlong2d(5)]\n",
    "        self.sex = [tlong2d(0), tlong2d(2)]\n",
    "        self.age = [tfloat2d(42), tfloat2d(79)]\n",
    "        self.race = [tlong2d(0), tlong2d(1)]\n",
    "        self.time = [tfloat2d(float('nan')), tfloat2d(491)]\n",
    "        self.event = [tlong2d(0), tlong2d(0)]\n",
    "        self.gene_ids = [\n",
    "            torch.stack([tlong(1), tlong(2), tlong(3)]),\n",
    "            torch.stack([tlong(4), tlong(5)])\n",
    "        ]\n",
    "        self.gene_emb = [\n",
    "            torch.stack([randemb(), randemb(), randemb()]),\n",
    "            torch.stack([randemb(),randemb()])\n",
    "            ]\n",
    "        self.maf = [\n",
    "            torch.stack([tfloat(.7), tfloat(.8), tfloat(.4)]),\n",
    "            torch.stack([tfloat(.05),tfloat(float('nan'))])\n",
    "            ]\n",
    "        self.focal_cna_ids = [\n",
    "            torch.stack([tlong(1)]),\n",
    "            torch.stack([tlong(1), tlong(4), tlong(7)])\n",
    "        ]\n",
    "        self.focal_cna = [\n",
    "            torch.stack([tlong(0)]),\n",
    "            torch.stack([tlong(0), tlong(1), tlong(0)])\n",
    "        ]\n",
    "        # self.broad_cna = [                                #broad is binary\n",
    "        #     torch.tensor([], dtype=torch.long),\n",
    "        #     torch.stack([tlong(0), tlong(2), tlong(3)])\n",
    "        # ]\n",
    "\n",
    "        #--seg data--#\n",
    "        self.seg_ids = [\n",
    "            torch.stack([tlong(1), tlong(2)]),\n",
    "            torch.stack([tlong(4), tlong(5), tlong(14), tlong(23)])\n",
    "        ]\n",
    "        self.seg_start = [                              #broad is emb                   \n",
    "            torch.stack([tfloat(12345*3e-9), tfloat(123456*3e-9)]),\n",
    "            torch.stack([tfloat(54321*3e-9), tfloat(999321*3e-9), tfloat(123456789*3e-9), tfloat(123456789*3e-9)])\n",
    "        ]\n",
    "        self.seg_end = [                                #broad is emb\n",
    "            torch.stack([tfloat(12459*3e-9), tfloat(125555*3e-9)]),\n",
    "            torch.stack([tfloat(64321*3e-9), tfloat(1111321*3e-9), tfloat(123467890*3e-9) ,tfloat(123467890*3e-9)])\n",
    "        ]\n",
    "        self.seg_mean = [                               #broad is emb\n",
    "            torch.stack([tfloat(0.5), tfloat(-0.7)]),\n",
    "            torch.stack([tfloat(0.1), tfloat(-0.2), tfloat(0.3), tfloat(0.3)])\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cancer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cancer = self.cancer[idx]\n",
    "        cancer_d = self.cancer_d[idx]\n",
    "        sex = self.sex[idx]\n",
    "        age = self.age[idx]\n",
    "        race = self.race[idx]\n",
    "        time = self.time[idx]\n",
    "        event = self.event[idx]\n",
    "        gene_id = self.gene_ids[idx]\n",
    "        gene_emb = self.gene_emb[idx]\n",
    "        maf = self.maf[idx]\n",
    "        focal_cna_id = self.focal_cna_ids[idx]\n",
    "        focal_cna = self.focal_cna[idx]\n",
    "        # broad_cna = self.broad_cna[idx] #broad is binary\n",
    "        seg_id = self.seg_ids[idx]\n",
    "        seg_start = self.seg_start[idx]\n",
    "        seg_end = self.seg_end[idx]\n",
    "        seg_mean = self.seg_mean[idx]\n",
    "        # return cancer, sex, age, time, event, gene_emb, maf, focal_cna, broad_cna                                 #broad is binary\n",
    "        return cancer, cancer_d, sex, age, race, time, event, gene_id, gene_emb, maf, focal_cna_id, focal_cna, seg_id, seg_start, seg_end, seg_mean  #broad is emb\n",
    "\n",
    "def collate_test(batch, config):\n",
    "    cancer =    torch.stack([item[0] for item in batch])\n",
    "    cancer_d =  torch.stack([item[1] for item in batch])\n",
    "    sex =       torch.stack([item[2] for item in batch])\n",
    "    age =       torch.stack([item[3] for item in batch])\n",
    "    race =      torch.stack([item[4] for item in batch])\n",
    "    time =      torch.stack([item[5] for item in batch])\n",
    "    event =     torch.stack([item[6] for item in batch])\n",
    "\n",
    "    def pad_and_mask(batch_, i, pad_val, mask = True):\n",
    "        unpadded_list = [item[i] for item in batch_]\n",
    "        padded_list = pad_sequence(unpadded_list, batch_first=True, padding_value=pad_val)\n",
    "        if mask:\n",
    "            lengths = [j.size(0) for j in unpadded_list]\n",
    "            max_length = padded_list.size(1)\n",
    "            # print('\\n-----------------')\n",
    "            # print('max_length',max_length)\n",
    "            # print('lengths',lengths)\n",
    "            # print('torch.arange(max_length).expand(len(lengths), max_length)\\n',torch.arange(max_length).expand(len(lengths), max_length))\n",
    "            mask = torch.arange(max_length).expand(len(lengths), max_length) >= torch.tensor(lengths).unsqueeze(1)\n",
    "            # print('torch.arange(max_length).expand(len(lengths), max_length) >= torch.tensor(lengths).unsqueeze(1)\\n', mask)\n",
    "            # print('-----------------\\n')\n",
    "            return padded_list, mask\n",
    "        return padded_list\n",
    "\n",
    "    gene_id =                       pad_and_mask(batch, 7, config.gene_id_vocab_size, mask = False)\n",
    "    gene_emb =                      pad_and_mask(batch, 8, 0, mask = False)\n",
    "    maf =                           pad_and_mask(batch, 9, 0, mask = False)\n",
    "    focal_cna_id =                  pad_and_mask(batch, 10, config.gene_id_vocab_size, mask = False)\n",
    "    focal_cna =                     pad_and_mask(batch, 11, config.focal_cna_vocab_size, mask = False)\n",
    "    # broad_cna, broad_cna_pad_mask = pad_and_mask(batch, 11, config.broad_cna_vocab_size) #broad is binary\n",
    "    \n",
    "    seg_id =                        pad_and_mask(batch, 12, config.seg_id_vocab_size, mask = False)\n",
    "    seg_start =                     pad_and_mask(batch, 13, 0, mask = False)                      #broad is emb\n",
    "    seg_end =                       pad_and_mask(batch, 14, 0, mask = False)        #broad is emb\n",
    "    seg_mean =                      pad_and_mask(batch, 15, 0, mask = False)        #broad is emb\n",
    "\n",
    "    # return (cancer, sex, age, time, event, gene_emb, maf, focal_cna, broad_cna, pad_mask) #broad is binary\n",
    "    return (cancer, cancer_d, sex, age, race, time, event, gene_id, gene_emb, maf, focal_cna_id, focal_cna, seg_id, seg_start, seg_end, seg_mean) #broad is emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add mutational burden\n",
    "## train model to predict effect of PARP inhibition\n",
    "## IMPLEMENT PERCENTILE RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ../labeled_data/label_mappings/label_mapping_SEX_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_RACE_somatt_data_df.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f20ab891a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dandreas/.conda/envs/esm3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config_Somatt' object has no attribute 'sex_nan_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     embed_surv: nn\u001b[38;5;241m.\u001b[39mModule \u001b[38;5;241m=\u001b[39m EmbedSurv_rbf_2heads\n\u001b[1;32m     32\u001b[0m config \u001b[38;5;241m=\u001b[39m Config_Somatt()\n\u001b[0;32m---> 33\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSomatt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset_test()\n\u001b[1;32m     37\u001b[0m collate_test_with_config \u001b[38;5;241m=\u001b[39m partial(collate_test, config \u001b[38;5;241m=\u001b[39m config)\n",
      "File \u001b[0;32m~/SomaticMutationLLM/model/somatt.py:211\u001b[0m, in \u001b[0;36mSomatt.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_cancer_type \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mcancer_type_vocab_size, config\u001b[38;5;241m.\u001b[39memb_dim)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_cancer_type_detailed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mcancer_type_detailed_vocab_size, config\u001b[38;5;241m.\u001b[39memb_dim)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msex_nan_idx \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msex_nan_idx\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_sex \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39msex_vocab_size, config\u001b[38;5;241m.\u001b[39memb_dim, padding_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msex_nan_idx)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_age \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mlin_proj(config, input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config_Somatt' object has no attribute 'sex_nan_idx'"
     ]
    }
   ],
   "source": [
    "sex_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_SEX_somatt_data_df.pkl')\n",
    "race_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_RACE_somatt_data_df.pkl')\n",
    "data_df = pd.read_pickle('../labeled_data/somatt_data_df.pkl')\n",
    "\n",
    "class Config_Somatt:\n",
    "    n_layer: int = 3\n",
    "    emb_dim: int = 640 #1152 esmC #1536 esm3 #640 esm2\n",
    "    input_dim: int = 640\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False\n",
    "    gene_id_vocab_size : int = 1433\n",
    "    cancer_type_vocab_size: int = 10\n",
    "    cancer_type_detailed_vocab_size: int = 33\n",
    "    pooling : str = 'mean'\n",
    "    norm_fn: nn.Module = nn.LayerNorm\n",
    "    position_embedding: bool = False\n",
    "    sex_label_map: dict = sex_label_mapping\n",
    "    sex_vocab_size: int =  len(sex_label_mapping)\n",
    "    race_label_map: dict = race_label_mapping\n",
    "    race_vocab_size: int = len(race_label_mapping)\n",
    "    num_heads: int = 1\n",
    "    lin_proj: nn.Module = LinProj\n",
    "    rbf_params = (0,1,16)\n",
    "    n_clin_vars: int = 6\n",
    "    maf_emb_dim: int = 16\n",
    "    seg_id_vocab_size: int = 46\n",
    "    broad_cna_vocab_size: int = 2   \n",
    "    focal_cna_vocab_size: int = 2   \n",
    "    event_vocab_size: int = 2\n",
    "    embed_surv: nn.Module = EmbedSurv_rbf_2heads\n",
    "\n",
    "config = Config_Somatt()\n",
    "model = Somatt(config)\n",
    "\n",
    "ds = Dataset_test()\n",
    "\n",
    "collate_test_with_config = partial(collate_test, config = config)\n",
    "loader = DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_test_with_config)\n",
    "\n",
    "for batch in loader:\n",
    "    mask = create_mask(2,3,probs=[.5,.5,.5])\n",
    "    output = model(batch, mask = mask)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ../aa/tumors.pkl\n",
      "loading data from ../data_processing/map_tumorBarcode_to_clinicalSampleIdx.pkl\n",
      "loading data from ../aa/assays.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_SEX_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_RACE_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_ARM_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_CANCER_TYPE_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_CANCER_TYPE_DETAILED_somatt_data_df.pkl\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164586 samples\n",
      "147659 unique patients\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 17, got 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset_Somatt(data_df, mut_embeddings, ref_embeddings, tumors, assays, device)\n\u001b[1;32m     69\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_somatt_with_config)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mtrain_masked_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mtrain_masked_loss\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 12\u001b[0m     cancer, cancer_d, sex, age, race, time, event, gene_id, gene_emb, maf, focal_cna_id, focal_cna, seg_id, seg_start, seg_end, seg_mean, seg_pad_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m     mask \u001b[38;5;241m=\u001b[39m create_mask (cancer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m, probs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m.5\u001b[39m,\u001b[38;5;241m.5\u001b[39m,\u001b[38;5;241m.5\u001b[39m])\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 17, got 16)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume these loss functions are defined:\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Example training loop snippet\n",
    "def train_masked_loss(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        cancer, cancer_d, sex, age, race, time, event, gene_id, gene_emb, maf, focal_cna_id, focal_cna, seg_id, seg_start, seg_end, seg_mean, seg_pad_mask = batch\n",
    "        mask = create_mask (cancer.size(0), 3, probs=[.5,.5,.5])\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch, mask=mask)\n",
    "        \n",
    "        cancer_logits = outputs[0]          # shape: [B, num_cancer_classes]\n",
    "        cancer_detailed_logits = outputs[1]   # shape: [B, num_cancer_detailed_classes]\n",
    "        survival_logits = outputs[5]          # shape: [B, 1] (or [B])\n",
    "        \n",
    "        mask_cancer = mask[:, 0].squeeze()          # shape: [B]\n",
    "        mask_cancer_detailed = mask[:, 1].squeeze()   # shape: [B]\n",
    "        mask_survival = mask[:, 2].squeeze()          # shape: [B]\n",
    "        \n",
    "        # Compute loss only over the masked tokens:\n",
    "        if mask_cancer.sum() > 0:\n",
    "            loss_cancer = classification_loss_fn(\n",
    "                cancer_logits[mask_cancer], \n",
    "                cancer[mask_cancer].long().squeeze()\n",
    "            )\n",
    "        else:\n",
    "            loss_cancer = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        if mask_cancer_detailed.sum() > 0:\n",
    "            loss_cancer_detailed = classification_loss_fn(\n",
    "                cancer_detailed_logits[mask_cancer_detailed], \n",
    "                cancer_d[mask_cancer_detailed].long().squeeze()\n",
    "            )\n",
    "        else:\n",
    "            loss_cancer_detailed = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # For survival, stack time and event into a tensor of shape [B, 2]\n",
    "        survival_targets = torch.cat([time, event], dim=1)\n",
    "        if mask_survival.sum() > 0:\n",
    "            masked_survival = survival_targets[mask_survival]\n",
    "            masked_risk = survival_logits[mask_survival].squeeze()\n",
    "\n",
    "            # Assume negative_log_partial_likelihood takes survival_targets and survival logits.\n",
    "            loss_survival = negative_log_partial_likelihood(\n",
    "                survival_targets[mask_survival],\n",
    "                survival_logits[mask_survival].squeeze()\n",
    "            )\n",
    "        else:\n",
    "            loss_survival = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Combine losses (you can also weight them if needed)\n",
    "        total_loss = loss_cancer + loss_cancer_detailed + loss_survival\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Batch Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "config = Config_Somatt()\n",
    "model = Somatt(config)\n",
    "collate_somatt_with_config = partial(collate_somatt, config = config)\n",
    "ds = Dataset_Somatt(data_df, mut_embeddings, ref_embeddings, tumors, assays, device)\n",
    "loader = DataLoader(ds, batch_size=100, shuffle=True, collate_fn=collate_somatt_with_config)\n",
    "\n",
    "train_masked_loss(model, loader, torch.optim.Adam(model.parameters()), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dandreas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ../aa/tumors.pkl\n",
      "loading data from ../data_processing/map_tumorBarcode_to_clinicalSampleIdx.pkl\n",
      "loading data from ../aa/assays.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_SEX_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_RACE_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_ARM_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_CANCER_TYPE_somatt_data_df.pkl\n",
      "loading data from ../labeled_data/label_mappings/label_mapping_CANCER_TYPE_DETAILED_somatt_data_df.pkl\n",
      "143530\n",
      "10000 samples\n",
      "9907 unique patients\n",
      "\n",
      "FOLD 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: 100%|██████████| 81/81 [01:01<00:00,  1.32it/s, Epoch=1/15, Loss: 2.2230] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 9.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Test Accuracy: 18.31%, Detailed: 13.76%, Survival C-Index: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: 100%|██████████| 81/81 [01:00<00:00,  1.34it/s, Epoch=2/15, Loss: 1.3604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Train Loss: 8.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Test Accuracy: 26.26%, Detailed: 18.61%, Survival C-Index: 0.8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: 100%|██████████| 81/81 [00:58<00:00,  1.37it/s, Epoch=3/15, Loss: 3.1847] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Train Loss: 8.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: 100%|██████████| 20/20 [00:06<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Test Accuracy: 32.37%, Detailed: 19.56%, Survival C-Index: 0.8319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING:  10%|▉         | 8/81 [00:05<00:52,  1.40it/s, Epoch=4/15, Loss: 9.0424]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m data_df \u001b[38;5;241m=\u001b[39m data_df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     66\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset_Somatt(data_df, mut_embeddings, ref_embeddings, tumors, assays, device)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtrain_somatt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSomatt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig_Somatt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcollate_somatt_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SomaticMutationLLM/model/somatt.py:524\u001b[0m, in \u001b[0;36mtrain_somatt\u001b[0;34m(modelClass, configClass, dataset, data_df, saveName, n_folds, test_size, num_epochs, lr, batch_size, device, collate_fn)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saveName \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     current_saveName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./best_models/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m saveName\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 524\u001b[0m \u001b[43mtrain_masked_loss_with_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_saveName\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SomaticMutationLLM/model/somatt.py:387\u001b[0m, in \u001b[0;36mtrain_masked_loss_with_test\u001b[0;34m(model, config, train_loader, test_loader, optimizer, device, num_epochs, saveName)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# --- Training Phase ---\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader),desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# for batch in tqdm(train_loader, desc=f\"TRAINING Epoch {epoch+1}/{num_epochs}\"):\u001b[39;00m\n\u001b[1;32m    389\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;66;03m# Unpack the batch (assumed collate returns 17 items)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/esm3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/esm3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/esm3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SomaticMutationLLM/model/somatt.py:128\u001b[0m, in \u001b[0;36mcollate_somatt\u001b[0;34m(batch, config)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_list\n\u001b[1;32m    127\u001b[0m gene_id \u001b[38;5;241m=\u001b[39m                       pad(batch, \u001b[38;5;241m7\u001b[39m, config\u001b[38;5;241m.\u001b[39mgene_id_vocab_size)\n\u001b[0;32m--> 128\u001b[0m gene_emb \u001b[38;5;241m=\u001b[39m                      \u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m maf \u001b[38;5;241m=\u001b[39m                           pad(batch, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    130\u001b[0m focal_cna_id \u001b[38;5;241m=\u001b[39m                  pad(batch, \u001b[38;5;241m10\u001b[39m, config\u001b[38;5;241m.\u001b[39mgene_id_vocab_size)\n",
      "File \u001b[0;32m~/SomaticMutationLLM/model/somatt.py:124\u001b[0m, in \u001b[0;36mcollate_somatt.<locals>.pad\u001b[0;34m(batch_, i, pad_val, mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad\u001b[39m(batch_, i, pad_val, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    123\u001b[0m     unpadded_list \u001b[38;5;241m=\u001b[39m [item[i] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch_]\n\u001b[0;32m--> 124\u001b[0m     padded_list \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpadded_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_list\n",
      "File \u001b[0;32m~/.conda/envs/esm3/lib/python3.10/site-packages/torch/nn/utils/rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[1;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from somatt import *\n",
    "from saveAndLoad import pickleLoad\n",
    "\n",
    "tumors = pickleLoad('../aa/tumors.pkl')\n",
    "map_tumorBarcode_to_clinicalSampleIdx = pickleLoad('../data_processing/map_tumorBarcode_to_clinicalSampleIdx.pkl')\n",
    "mut_embeddings = np.load('../aa/canonical_mut_norm_embeddings_esm2.npy')\n",
    "ref_embeddings = np.load('../aa/canonical_ref_embeddings_esm2.npy')\n",
    "assays = pickleLoad('../aa/assays.pkl')\n",
    "device = 'cuda:1'\n",
    "sex_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_SEX_somatt_data_df.pkl')\n",
    "race_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_RACE_somatt_data_df.pkl')\n",
    "arm_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_ARM_somatt_data_df.pkl')\n",
    "cancer_type_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_CANCER_TYPE_somatt_data_df.pkl')\n",
    "cancer_type_detailed_label_mapping = pickleLoad('../labeled_data/label_mappings/label_mapping_CANCER_TYPE_DETAILED_somatt_data_df.pkl')\n",
    "n_cancer_types = len(cancer_type_label_mapping)\n",
    "n_cancer_type_d = len(cancer_type_detailed_label_mapping)\n",
    "n_seg_id = len(arm_label_mapping)\n",
    "sex_nan_idx = len(sex_label_mapping)-1\n",
    "assert math.isnan(sex_label_mapping[sex_nan_idx])\n",
    "race_nan_idx = len(race_label_mapping)-1\n",
    "assert math.isnan(race_label_mapping[race_nan_idx])\n",
    "\n",
    "class Config_Somatt:\n",
    "    n_layer: int = 3\n",
    "    emb_dim: int = 640 #1152 esmC #1536 esm3 #640 esm2\n",
    "    input_dim: int = 640\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "    gene_id_vocab_size : int = 1433\n",
    "    cancer_type_vocab_size: int = n_cancer_types\n",
    "    cancer_type_detailed_vocab_size: int = n_cancer_type_d\n",
    "    norm_fn: nn.Module = nn.LayerNorm\n",
    "    position_embedding: bool = False\n",
    "    sex_nan_idx: int = sex_nan_idx\n",
    "    sex_vocab_size: int =  len(sex_label_mapping)\n",
    "    race_nan_idx: int = race_nan_idx\n",
    "    race_vocab_size: int = len(race_label_mapping)\n",
    "    num_heads: int = 1\n",
    "    lin_proj: nn.Module = LinProj\n",
    "    rbf_params = (0,1,16)\n",
    "    n_clin_vars: int = 6\n",
    "    maf_emb_dim: int = 16\n",
    "    seg_id_vocab_size: int = n_seg_id\n",
    "    pool_gene: bool = False\n",
    "    pool_seg: bool = True\n",
    "    broad_cna_vocab_size: int = 2   \n",
    "    focal_cna_vocab_size: int = 2   \n",
    "    event_vocab_size: int = 2\n",
    "    embed_surv: nn.Module = EmbedSurv_rbf_2heads\n",
    "\n",
    "# Example usage:\n",
    "config = Config_Somatt()  # Your defined config class for Somatt.\n",
    "model = Somatt(config)\n",
    "collate_somatt_with_config = partial(collate_somatt, config=config)\n",
    "\n",
    "data_df = pd.read_pickle('../labeled_data/somatt_data_df.pkl')\n",
    "label_counts = data_df['CANCER_TYPE'].value_counts().to_dict()\n",
    "min_cancer_type = .01\n",
    "filter_rarity = lambda x: label_counts[x]>=min_cancer_type\n",
    "min_cancer_type = int(len(data_df)*min_cancer_type)\n",
    "data_df = data_df[data_df['CANCER_TYPE'].apply(filter_rarity)]\n",
    "print(len(data_df))\n",
    "data_df = data_df.sample(n=10000, random_state=42)\n",
    "\n",
    "ds = Dataset_Somatt(data_df, mut_embeddings, ref_embeddings, tumors, assays, device)\n",
    "\n",
    "train_somatt(Somatt, Config_Somatt, ds, data_df, batch_size = 100, saveName=None, test_size=0.2, num_epochs=15, lr=1e-4, device='cuda:1', collate_fn = collate_somatt_with_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_tensor = lambda x: torch.tensor(x, dtype=torch.long)\n",
    "long_tensor([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
