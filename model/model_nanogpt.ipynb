{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dandreas/.conda/envs/deepV_a100/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/dandreas/.conda/envs/deepV_a100/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from saveAndLoad import *\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8, bias = None):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.norm(keepdim=True, dim=-1) * (x.size(-1) ** -0.5)\n",
    "        return self.scale * (x / (norm + self.eps))\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = config.norm_fn(config.n_embd, bias=config.bias)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = config.norm_fn(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, positions = None, mask = None):\n",
    "        x = x + self.attn(self.ln_1(x), positions, mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        if self.use_dropout: x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.pos_embs = nn.Embedding(config.max_len, config.n_embd)\n",
    "\n",
    "    def forward(self, x, positions = None, mask = None):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        if positions is not None:\n",
    "            pos_emb = self.pos_embs(positions)\n",
    "            x = x + pos_emb\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(att.size())\n",
    "            att = att.masked_fill(mask == 0, -torch.inf)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pooling = config.pooling\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = config.norm_fn(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        # self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, positions=None):\n",
    "        if self.pooling == 'cls':\n",
    "            batch_size = x.size(0)\n",
    "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Expand CLS token for each sequence in the batch\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        mask = (x != 0).any(dim=-1).float()  # Mask out zero vectors\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x, positions, mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            classifier_input = x[:, 0, :].view(-1, self.input_dim)\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = mask.unsqueeze(-1).expand_as(x)\n",
    "            classifier_input = (x * mask).sum(dim=-2) / mask.sum(dim=-2)\n",
    "        elif self.pooling == 'max':\n",
    "            mask = mask.unsqueeze(-1).expand_as(x)\n",
    "            classifier_input, _ = (x * mask).max(dim=-2)\n",
    "        \n",
    "        if self.num_labels==1: classifier_input = self.norm(classifier_input) #survival analysis\n",
    "\n",
    "        logits = self.lm_head(classifier_input)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 \t BINARYdata_CANCER_TYPE_0MinMutations_1696MinCancerType.csv\n",
      "24 \t BINARYdata_CANCER_TYPE_0MinMutations_169MinCancerType.csv\n",
      "1 \t BINARYdata_CANCER_TYPE_3MinMutations_1696MinCancerType.csv\n",
      "32 \t BINARYdata_CANCER_TYPE_3MinMutations_169MinCancerType.csv\n",
      "12 \t BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_1696MinCancerType.csv\n",
      "26 \t BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_169MinCancerType.csv\n",
      "10 \t BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_1696MinCancerType.csv\n",
      "30 \t BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_169MinCancerType.csv\n",
      "23 \t data_CANCER_TYPE_0MinMutations_1696MinCancerType.csv\n",
      "25 \t data_CANCER_TYPE_0MinMutations_169MinCancerType.csv\n",
      "41 \t data_CANCER_TYPE_3MinMutations_1696MinCancerType.csv\n",
      "17 \t data_CANCER_TYPE_3MinMutations_169MinCancerType.csv\n",
      "11 \t data_CANCER_TYPE_DETAILED_0MinMutations_1696MinCancerType.csv\n",
      "28 \t data_CANCER_TYPE_DETAILED_0MinMutations_169MinCancerType.csv\n",
      "4 \t data_CANCER_TYPE_DETAILED_3MinMutations_1696MinCancerType.csv\n",
      "47 \t data_CANCER_TYPE_DETAILED_3MinMutations_169MinCancerType.csv\n",
      "35 \t label_mappings\n",
      "45 \t top100_BINARYdata_CANCER_TYPE_0MinMutations_1604MinCancerType.csv\n",
      "22 \t top100_BINARYdata_CANCER_TYPE_0MinMutations_160MinCancerType.csv\n",
      "33 \t top100_BINARYdata_CANCER_TYPE_3MinMutations_1604MinCancerType.csv\n",
      "0 \t top100_BINARYdata_CANCER_TYPE_3MinMutations_160MinCancerType.csv\n",
      "2 \t top100_BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_1604MinCancerType.csv\n",
      "43 \t top100_BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_160MinCancerType.csv\n",
      "6 \t top100_BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_1604MinCancerType.csv\n",
      "21 \t top100_BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_160MinCancerType.csv\n",
      "38 \t top100_data_CANCER_TYPE_0MinMutations_1604MinCancerType.csv\n",
      "40 \t top100_data_CANCER_TYPE_0MinMutations_160MinCancerType.csv\n",
      "36 \t top100_data_CANCER_TYPE_3MinMutations_1604MinCancerType.csv\n",
      "31 \t top100_data_CANCER_TYPE_3MinMutations_160MinCancerType.csv\n",
      "3 \t top100_data_CANCER_TYPE_DETAILED_0MinMutations_1604MinCancerType.csv\n",
      "20 \t top100_data_CANCER_TYPE_DETAILED_0MinMutations_160MinCancerType.csv\n",
      "44 \t top100_data_CANCER_TYPE_DETAILED_3MinMutations_1604MinCancerType.csv\n",
      "16 \t top100_data_CANCER_TYPE_DETAILED_3MinMutations_160MinCancerType.csv\n",
      "34 \t top10_BINARYdata_CANCER_TYPE_0MinMutations_1213MinCancerType.csv\n",
      "46 \t top10_BINARYdata_CANCER_TYPE_0MinMutations_121MinCancerType.csv\n",
      "15 \t top10_BINARYdata_CANCER_TYPE_3MinMutations_1213MinCancerType.csv\n",
      "29 \t top10_BINARYdata_CANCER_TYPE_3MinMutations_121MinCancerType.csv\n",
      "48 \t top10_BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_1213MinCancerType.csv\n",
      "42 \t top10_BINARYdata_CANCER_TYPE_DETAILED_0MinMutations_121MinCancerType.csv\n",
      "19 \t top10_BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_1213MinCancerType.csv\n",
      "8 \t top10_BINARYdata_CANCER_TYPE_DETAILED_3MinMutations_121MinCancerType.csv\n",
      "39 \t top10_data_CANCER_TYPE_0MinMutations_1696MinCancerType.csv\n",
      "9 \t top10_data_CANCER_TYPE_0MinMutations_169MinCancerType.csv\n",
      "14 \t top10_data_CANCER_TYPE_3MinMutations_1696MinCancerType.csv\n",
      "7 \t top10_data_CANCER_TYPE_3MinMutations_169MinCancerType.csv\n",
      "5 \t top10_data_CANCER_TYPE_DETAILED_0MinMutations_1696MinCancerType.csv\n",
      "27 \t top10_data_CANCER_TYPE_DETAILED_0MinMutations_169MinCancerType.csv\n",
      "18 \t top10_data_CANCER_TYPE_DETAILED_3MinMutations_1696MinCancerType.csv\n",
      "13 \t top10_data_CANCER_TYPE_DETAILED_3MinMutations_169MinCancerType.csv\n",
      "\n",
      " data_CANCER_TYPE_3MinMutations_1696MinCancerType.csv\n"
     ]
    }
   ],
   "source": [
    "from custom_dataset import *\n",
    "\n",
    "# LOAD DATA\n",
    "# canonical_ave_embeddings_esm2 = np.load('../aa/canonical_mut_average_embeddings_esm2.npy')\n",
    "data_dir = '../labeled_data/'\n",
    "labeled_data = os.listdir(data_dir)\n",
    "for ni,i in sorted(zip(labeled_data,range(len(labeled_data)))):print(i,'\\t',ni)\n",
    "data = labeled_data[41]\n",
    "print('\\n',data)\n",
    "data_df = pd.read_csv(data_dir+data)\n",
    "data = data_df['idxs'].values\n",
    "labels = torch.tensor(data_df['int_label'].values,dtype=torch.long)\n",
    "nlabels = len(data_df['int_label'].unique())\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset_MutationList(data_df, canonical_ave_embeddings_esm2,device)\n",
    "\n",
    "# Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=100, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# TEST/TRAIN SPLIT\n",
    "test_size = .2\n",
    "random_state = 42\n",
    "batch_size = 1\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18109837773478196"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# majority classifier\n",
    "print(len(data_df['int_label'].unique()))\n",
    "sorted(data_df['int_label'].value_counts(),reverse=True)[0]/len(data_df['int_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n labels: 20\n",
      "number of parameters: 5.86M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING:   0%|          | 0/80628 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'positions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     40\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m    165\u001b[0m mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Mask out zero vectors\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 168\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x, \u001b[43mpositions\u001b[49m, mask)\n\u001b[1;32m    169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positions' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 30\n",
    "    vocab_size: int = 17 #n_labels\n",
    "    n_layer: int = 1 #12\n",
    "    n_head: int = 1 #10\n",
    "    n_embd: int = 640\n",
    "    dropout: float = 0.0\n",
    "    max_len: int = 1448\n",
    "    norm_fn: nn.Module =  LayerNorm\n",
    "    pooling: str = 'mean'\n",
    "    bias: bool = False # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "print('n labels:',nlabels)\n",
    "config = GPTConfig()\n",
    "config.vocab_size = nlabels\n",
    "config.pooling = 'mean'\n",
    "config.norm_fn = RMSNorm\n",
    "\n",
    "model = GPT(config)\n",
    "model.to('cuda:1')\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(enumerate(train_loader), total=len(train_loader),desc='TRAINING') as pbar:\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix({'Epoch':f'{epoch+1}/{num_epochs}, Loss: {loss.item():.4f}'})\n",
    "            if batch_idx % 20000 == 0:\n",
    "                print('')\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(test_loader,desc='TESTING'):\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%, ({correct} of {total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepV_a100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
